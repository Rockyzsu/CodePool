### list
用法： scrapy check [-l] <spider>

需要工程： 是

描述： 运行相关检查

### list源码分析
在scrapy.commands.check.py中找到如下代码： 
```python 
    def run(self, args, opts):
        # load contracts
        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))
        conman = ContractsManager(load_object(c) for c in contracts)
        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)
        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)

        # contract requests
        contract_reqs = defaultdict(list)

        spider_loader = self.crawler_process.spider_loader

        for spidername in args or spider_loader.list():
            spidercls = spider_loader.load(spidername)
            spidercls.start_requests = lambda s: conman.from_spider(s, result)

            tested_methods = conman.tested_methods_from_spidercls(spidercls)
            if opts.list:
                for method in tested_methods:
                    contract_reqs[spidercls.name].append(method)
            elif tested_methods:
                self.crawler_process.crawl(spidercls)

        # start checks
        if opts.list:
            for spider, methods in sorted(contract_reqs.items()):
                if not methods and not opts.verbose:
                    continue
                print(spider)
                for method in sorted(methods):
                    print('  * %s' % method)
        else:
            start = time.time()
            self.crawler_process.start()
            stop = time.time()

            result.printErrors()
            result.printSummary(start, stop)
            self.exitcode = int(not result.wasSuccessful())
```
运行爬虫，获取错误信息和汇总信息。

### list 
用法： scrapy list
需要工程： 是
描述： 列出当前工程目录下的可用爬虫，一行一个
使用样例 
``` cmd
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy list
test_base
test_crawl
test_csvfeed
test_xmlfeed
```
### list源码分析
在scrapy.commands.list.py找到如下代码
```python
    def run(self, args, opts):
        for s in sorted(self.crawler_process.spider_loader.list()):
            print(s)
```
我们可以看到这里是获取到spider加载器的list方法，排序输出的结果。


### edit
用法： scrap edit <spider>

需要工程： 是

描述： 使用一个编辑器（EDITOR环境变量设置)编辑爬虫文本内容

这个命令不怎么常用的，一般我们都是用开发工具编辑， 这个命令一般是在linux终端可能会使用的。EDITOR在window下默认是记事本，在linux下默认是vi。

### edit源码分析
在scrapy.commands.edit.py中，找到如下代码
```python
    def run(self, args, opts):
        if len(args) != 1:
            raise UsageError()

        editor = self.settings['EDITOR']
        try:
            spidercls = self.crawler_process.spider_loader.load(args[0])
        except KeyError:
            return self._err("Spider not found: %s" % args[0])

        sfile = sys.modules[spidercls.__module__].__file__
        sfile = sfile.replace('.pyc', '.py')
        self.exitcode = os.system('%s "%s"' % (editor, sfile))
```
可以看到获取到editor的环境变量值， 获取需要编辑的py文件（也就是爬虫文件），然后os.system方法，使用指定编辑器打开py文件。

### fetch 
用法： scrapy fetch <url>

需要工程： 否

描述： 下载制定的url内容，写html内容到标准输出流

支持一些选项的

* --spider=强制使用制定的爬虫
* --headers: 打印http headers信息，而不是默认的html源码内容
* --no-redirect: 不允许重定向

### fetch 源码分析
在scrapy.commmands.fetch.py文件找到如下代码： 
```python

    def run(self, args, opts):
        if len(args) != 1 or not is_url(args[0]):
            raise UsageError()
        cb = lambda x: self._print_response(x, opts)
        request = Request(args[0], callback=cb, dont_filter=True)
        # by default, let the framework handle redirects,
        # i.e. command handles all codes expect 3xx
        if not opts.no_redirect:
            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))
        else:
            request.meta['handle_httpstatus_all'] = True

        spidercls = DefaultSpider
        spider_loader = self.crawler_process.spider_loader
        if opts.spider:
            spidercls = spider_loader.load(opts.spider)
        else:
            spidercls = spidercls_for_request(spider_loader, request, spidercls)
        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])
        self.crawler_process.start()
```
可以看出， 这个方法的调用和我们crawl调用差不多的， 这里使用的DefaultSpider,crawl方法调用会使用工程的爬虫， 初步推测这个DefaultSpider爬虫的parse方法只是打印response.body信息。

