### startproject
 * 用法:scrapy startproject <project_name> [project_dir]

* 需要工程：否

在`project_dir`目录创建一个项目， 项目名字为`project_name`，如果`project_dir`没有指定的话，`project_dir`会和`project_name`一样。其中工程目录支持相对和绝对路径的。

我们在demo目录测试下看看效果。
```cmd
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo>scrapy startproject test01 dir01
New Scrapy project 'test01', using template directory 'C:\\Program Files\\Anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:
    e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\dir01

You can start your first spider with:
    cd dir01
    scrapy genspider example example.com

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo>scrapy startproject test02
New Scrapy project 'test02', using template directory 'C:\\Program Files\\Anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:
    e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02

You can start your first spider with:
    cd test02
    scrapy genspider example example.com
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo>tree
卷 新加卷 的文件夹 PATH 列表
卷序列号为 000000C7 D20B:7155
E:.
├─dir01
│  └─test01
│      ├─spiders
│      │  └─__pycache__
│      └─__pycache__
├─quotesbot-master
│  └─quotesbot
│      └─spiders
├─test02
│  └─test02
│      ├─spiders
│      │  └─__pycache__
│      └─__pycache__
└─tutorial
    └─tutorial
        ├─spiders
        │  └─__pycache__
        └─__pycache__
```
我们发现，我们没有指定工程目录的话，工程目录默认和工程名字一样了，如果我们指定了，就使用我们指定的工程目录。
### startproject 源码分析
这个源码分析的部分在03,04的文档中已经提到了。这里就不写了。

### genspider

* 用法: scrapy genspider [-t template] <name> <domain>
* 需要工程: 否
name指定爬虫的名字，domain会在爬虫文件中生成allowed_domains和start_urls爬虫属性。

使用样例
```
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
```
我们可以看到一共提供了4个模板。
我们在样例文件中分别创建者4个爬虫类，看看到底是啥。
```
C:\Users\Administrator>cd e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02

C:\Users\Administrator>e:

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>dir
 驱动器 E 中的卷是 新加卷
 卷的序列号是 D20B-7155

 e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02 的目录

2017/10/19  11:51    <DIR>          .
2017/10/19  11:51    <DIR>          ..
2017/10/19  11:51               256 scrapy.cfg
2017/10/19  11:51    <DIR>          test02
               1 个文件            256 字节
               3 个目录 172,873,003,008 可用字节

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t basic test_base www.baidu.com
Created spider 'test_base' using template 'basic' in module:
  test02.spiders.test_base

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t crawl test_crawl www.baidu.com
Created spider 'test_crawl' using template 'crawl' in module:
  test02.spiders.test_crawl

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t csvfeed test_csvfeed www.baidu.com
Created spider 'test_csvfeed' using template 'csvfeed' in module:
  test02.spiders.test_csvfeed

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t xmlfeed test_xmlfeed www.baidu.com
Created spider 'test_xmlfeed' using template 'xmlfeed' in module:
  test02.spiders.test_xmlfeed
```
执行类似如上的操作。我们可以看到在工程目录下生成了几个文件分别看看

base模板结果
``` python
# -*- coding: utf-8 -*-
import scrapy


class TestBaseSpider(scrapy.Spider):
    name = "test_base"
    allowed_domains = ["www.baidu.com"]
    start_urls = ['http://www.baidu.com/']

    def parse(self, response):
        pass

```
crawl模板结果
```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class TestCrawlSpider(CrawlSpider):
    name = 'test_crawl'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i

```
csvfeed模板结果
```python
# -*- coding: utf-8 -*-
from scrapy.spiders import CSVFeedSpider


class TestCsvfeedSpider(CSVFeedSpider):
    name = 'test_csvfeed'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/feed.csv']
    # headers = ['id', 'name', 'description', 'image_link']
    # delimiter = '\t'

    # Do any adaptations you need here
    #def adapt_response(self, response):
    #    return response

    def parse_row(self, response, row):
        i = {}
        #i['url'] = row['url']
        #i['name'] = row['name']
        #i['description'] = row['description']
        return i

```
xmlfeed模板结果
```python
# -*- coding: utf-8 -*-
from scrapy.spiders import XMLFeedSpider


class TestXmlfeedSpider(XMLFeedSpider):
    name = 'test_xmlfeed'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/feed.xml']
    iterator = 'iternodes' # you can change this; see the docs
    itertag = 'item' # change it accordingly

    def parse_node(self, response, selector):
        i = {}
        #i['url'] = selector.select('url').extract()
        #i['name'] = selector.select('name').extract()
        #i['description'] = selector.select('description').extract()
        return i

```
我们这里可以看出来这4个模板的不同的用途，
* csvfeed： 使用parse_row去一行一行解析csv的行数据
* xmlfeed: 使用parse_node方法解析xml的每个节点。
* crawl:  设定有rule，方便匹配指定格式的url去指定解析方法，
* base: 适合入门网址使用。

### genspider源码分析
在scrapy.commands.genspider.py文件中有如下代码
``` python
          def run(self, args, opts):
        if opts.list:
            self._list_templates()
            return
        if opts.dump:
            template_file = self._find_template(opts.dump)
            if template_file:
                with open(template_file, "r") as f:
                    print(f.read())
            return
        if len(args) != 2:
            raise UsageError()

        name, domain = args[0:2]
        module = sanitize_module_name(name)

        if self.settings.get('BOT_NAME') == module:
            print("Cannot create a spider with the same name as your project")
            return

        try:
            spidercls = self.crawler_process.spider_loader.load(name)
        except KeyError:
            pass
        else:
            # if spider already exists and not --force then halt
            if not opts.force:
                print("Spider %r already exists in module:" % name)
                print("  %s" % spidercls.__module__)
                return
        template_file = self._find_template(opts.template)
        if template_file:
            self._genspider(module, name, domain, opts.template, template_file)
            if opts.edit:
                self.exitcode = os.system('scrapy edit "%s"' % name)

    def _genspider(self, module, name, domain, template_name, template_file):
        """Generate the spider module, based on the given template"""
        tvars = {
            'project_name': self.settings.get('BOT_NAME'),
            'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),
            'module': module,
            'name': name,
            'domain': domain,
            'classname': '%sSpider' % ''.join(s.capitalize() \
                for s in module.split('_'))
        }
        if self.settings.get('NEWSPIDER_MODULE'):
            spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])
            spiders_dir = abspath(dirname(spiders_module.__file__))
        else:
            spiders_module = None
            spiders_dir = "."
        spider_file = "%s.py" % join(spiders_dir, module)
        shutil.copyfile(template_file, spider_file)
        render_templatefile(spider_file, **tvars)
        print("Created spider %r using template %r " % (name, \
            template_name), end=('' if spiders_module else '\n'))
        if spiders_module:
            print("in module:\n  %s.%s" % (spiders_module.__name__, module))
```
我们整理下这些代码，简单的看，现获取到系统模板文件，执行copy操作，然后按照一定的替换规则去替换模板文件的内容。最后打印创建成功的信息。

### crawl
用法： scrapy crawl <spider>
需要工程： 是
用途： 使用爬虫开始抓取
这个命令相对还是使用度比较高的。
### crawl源码分析
在scrapy.commands.crawl.py文件中，我们发现如下代码
```python 
    def run(self, args, opts):
        if len(args) < 1:
            raise UsageError()
        elif len(args) > 1:
            raise UsageError("running 'scrapy crawl' with more than one spider is no longer supported")
        spname = args[0]

        self.crawler_process.crawl(spname, **opts.spargs)
        self.crawler_process.start()
```
我们看到了使用crawl方法和start方法。我们定诶到scrapy.crawler.py文件
``` python 
    def start(self, stop_after_crawl=True):
        """
        This method starts a Twisted `reactor`_, adjusts its pool size to
        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based
        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.

        If `stop_after_crawl` is True, the reactor will be stopped after all
        crawlers have finished, using :meth:`join`.

        :param boolean stop_after_crawl: stop or not the reactor when all
            crawlers have finished
        """
        if stop_after_crawl:
            d = self.join()
            # Don't start the reactor if the deferreds are already fired
            if d.called:
                return
            d.addBoth(self._stop_reactor)

        reactor.installResolver(self._get_dns_resolver())
        tp = reactor.getThreadPool()
        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))
        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
        reactor.run(installSignalHandlers=False)  # blocking call


            def crawl(self, crawler_or_spidercls, *args, **kwargs):
        """
        Run a crawler with the provided arguments.

        It will call the given Crawler's :meth:`~Crawler.crawl` method, while
        keeping track of it so it can be stopped later.

        If `crawler_or_spidercls` isn't a :class:`~scrapy.crawler.Crawler`
        instance, this method will try to create one using this parameter as
        the spider class given to it.

        Returns a deferred that is fired when the crawling is finished.

        :param crawler_or_spidercls: already created crawler, or a spider class
            or spider's name inside the project to create it
        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,
            :class:`~scrapy.spiders.Spider` subclass or string

        :param list args: arguments to initialize the spider

        :param dict kwargs: keyword arguments to initialize the spider
        """
        crawler = self.create_crawler(crawler_or_spidercls)
        return self._crawl(crawler, *args, **kwargs)

    def _crawl(self, crawler, *args, **kwargs):
        self.crawlers.add(crawler)
        d = crawler.crawl(*args, **kwargs)
        self._active.add(d)

        def _done(result):
            self.crawlers.discard(crawler)
            self._active.discard(d)
            return result

        return d.addBoth(_done)
```
我们在CrawlerProcess类（继承了CrawlerRunner）中找到start方法，在CrawlerRunner类中找到crawl方法。具体代码我这里也是也是迷迷糊糊，就不多说了。 有兴趣的小伙伴可以读读这个部分的代码。

