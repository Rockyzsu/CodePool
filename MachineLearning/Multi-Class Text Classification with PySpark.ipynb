{"cells":[{"cell_type":"code","source":["# source : https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#Loading a CSV file is straightforward with Spark csv packages.\n#sc =SparkContext()\nsqlContext = SQLContext(sc)\ndata = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('FileStore/tables/train.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#Remove the columns we do not need and have a look the first five rows:\ndrop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\ndata = data.select([column for column in data.columns if column not in drop_list])\ndata.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+--------------------+\n      Category|            Descript|\n+--------------+--------------------+\n      WARRANTS|      WARRANT ARREST|\nOTHER OFFENSES|TRAFFIC VIOLATION...|\nOTHER OFFENSES|TRAFFIC VIOLATION...|\n LARCENY/THEFT|GRAND THEFT FROM ...|\n LARCENY/THEFT|GRAND THEFT FROM ...|\n+--------------+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["#Apply printSchema() on the data which will print the schema in a tree format:\ndata.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Category: string (nullable = true)\n-- Descript: string (nullable = true)\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["#Top 20 crime categories:\nfrom pyspark.sql.functions import col\ndata.groupBy(\"Category\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+\n            Category| count|\n+--------------------+------+\n       LARCENY/THEFT|174900|\n      OTHER OFFENSES|126182|\n        NON-CRIMINAL| 92304|\n             ASSAULT| 76876|\n       DRUG/NARCOTIC| 53971|\n       VEHICLE THEFT| 53781|\n           VANDALISM| 44725|\n            WARRANTS| 42214|\n            BURGLARY| 36755|\n      SUSPICIOUS OCC| 31414|\n      MISSING PERSON| 25989|\n             ROBBERY| 23000|\n               FRAUD| 16679|\nFORGERY/COUNTERFE...| 10609|\n     SECONDARY CODES|  9985|\n         WEAPON LAWS|  8555|\n        PROSTITUTION|  7484|\n            TRESPASS|  7326|\n     STOLEN PROPERTY|  4540|\nSEX OFFENSES FORC...|  4388|\n+--------------------+------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Model Pipeline\nSpark Machine Learning Pipelines API is similar to Scikit-Learn. Our pipeline includes three steps:\n\n1. **regexTokenizer**: Tokenization (with Regular Expression) \n2. **stopwordsRemover**: Remove Stop Words\n3. **countVectors**: Count vectors (“document-term vectors”)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\nfrom pyspark.ml.classification import LogisticRegression\n# regular expression tokenizer\nregexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n# stop words\nadd_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \nstopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n# bag of words count\ncountVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## StringIndexer\nStringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0.\n\nIn our case, the label column (Category) will be encoded to label indices, from 0 to 32; the most frequent label (LARCENY/THEFT) will be indexed as 0."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nlabel_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\npipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n# Fit the pipeline to training documents.\npipelineFit = pipeline.fit(data)\ndataset = pipelineFit.transform(data)\ndataset.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n      Category|            Descript|               words|            filtered|            features|label|\n+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(809,[17,32],[1.0...|  7.0|\nOTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\nOTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n+--------------+--------------------+--------------------+--------------------+--------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["dataset.select(['features']).head(2)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">33</span><span class=\"ansired\">]: </span>\n[Row(features=SparseVector(809, {17: 1.0, 32: 1.0})),\n Row(features=SparseVector(809, {11: 1.0, 17: 1.0, 35: 1.0}))]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Partition Training & Test sets"],"metadata":{}},{"cell_type":"code","source":["# set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint(\"Training Dataset Count: \" + str(trainingData.count()))\nprint(\"Test Dataset Count: \" + str(testData.count()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training Dataset Count: 614666\nTest Dataset Count: 263383\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["##Model Training and Evaluation\n\n**Logistic Regression using Count Vector Features**\n\nOur model will make predictions and score on the test set; we then look at the top 10 predictions from the highest probability."],"metadata":{}},{"cell_type":"code","source":["lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\nlrModel = lr.fit(trainingData)\npredictions = lrModel.transform(testData)\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------------+-------------+------------------------------+-----+----------+\n                      Descript|     Category|                   probability|label|prediction|\n+------------------------------+-------------+------------------------------+-----+----------+\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8711581002180206,0.02115...|  0.0|       0.0|\n+------------------------------+-------------+------------------------------+-----+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">36</span><span class=\"ansired\">]: </span>0.9725282146509521\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["**Logistic Regression using TF-IDF Features**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF\nhashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\npipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\npipelineFit = pipeline.fit(data)\ndataset = pipelineFit.transform(data)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nlr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\nlrModel = lr.fit(trainingData)\npredictions = lrModel.transform(testData)\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------------+-------------+------------------------------+-----+----------+\n                      Descript|     Category|                   probability|label|prediction|\n+------------------------------+-------------+------------------------------+-----+----------+\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\nTHEFT, BICYCLE, &lt;$50, NO SE...|LARCENY/THEFT|[0.8730538827818622,0.02078...|  0.0|       0.0|\n+------------------------------+-------------+------------------------------+-----+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">38</span><span class=\"ansired\">]: </span>0.9722954523853476\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["**The result is the same.**"],"metadata":{}},{"cell_type":"markdown","source":["**Cross-Validation**\n\nLet’s now try cross-validation to tune our hyper parameters, and we will only tune the count vectors Logistic Regression."],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\npipelineFit = pipeline.fit(data)\ndataset = pipelineFit.transform(data)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nlr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n             .build())\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=5)\ncvModel = cv.fit(trainingData)\n\npredictions = cvModel.transform(testData)\n# Evaluate best model\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">39</span><span class=\"ansired\">]: </span>0.9919124901837848\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["**Naive Bayes**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes(smoothing=1)\nmodel = nb.fit(trainingData)\npredictions = model.transform(testData)\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------+-------------+------------------------------+-----+----------+\n                    Descript|     Category|                   probability|label|prediction|\n+----------------------------+-------------+------------------------------+-----+----------+\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\nPETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9999999999837754,1.54986...|  0.0|       0.0|\n+----------------------------+-------------+------------------------------+-----+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">41</span><span class=\"ansired\">]: </span>0.9939847228864086\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["**Random Forest**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", \\\n                            featuresCol=\"features\", \\\n                            numTrees = 100, \\\n                            maxDepth = 4, \\\n                            maxBins = 32)\n# Train model with Training Data\nrfModel = rf.fit(trainingData)\npredictions = rfModel.transform(testData)\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------+-------------+------------------------------+-----+----------+\n                    Descript|     Category|                   probability|label|prediction|\n+----------------------------+-------------+------------------------------+-----+----------+\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\nGRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6402649319141156,0.06141...|  0.0|       0.0|\n+----------------------------+-------------+------------------------------+-----+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">43</span><span class=\"ansired\">]: </span>0.6490477620735787\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Random forest is a very good, robust and versatile method, however it’s no mystery that for high-dimensional sparse data it’s not a best choice.\n\n**随机森林是一种非常好的，强大而通用的方法，但对于高维稀疏数据来说，它并不是最好的选择。**\n\nIt is obvious that Logistic Regression will be our model in this experiment, with cross validation.\n\n**很明显Logistic回归将是我们在这个实验中的模型，具有交叉验证。**\n\nThis brings us to the end of the article. Source code that create this post can be found on Github. I look forward to hear any feedback or questions."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"name":"Multi-Class Text Classification with PySpark","notebookId":4259795506935810},"nbformat":4,"nbformat_minor":0}
