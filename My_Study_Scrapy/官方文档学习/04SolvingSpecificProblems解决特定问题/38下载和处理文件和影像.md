## 下载和处理文件和影像

files pipeline 和Images pipeline 相对pipeline添加了新的功能
* 避免下载最近的。
* 指定存储

影像pipeline新的功能
* 转化影像为jpg,rgb
* 缩略图生成
* 检查影像官渡和高度，确保一致性

### 使用文件管道FilePipeline
filepipeline工作流程

1. 在你的爬虫里面，把你的文件的url放到file_urls字段中
2. 这个item从爬虫返回，然后去到itempipeline中去
3. item到达filespipeline，file_urls字段中的文件url被调度去下载，有这更高的优先级，这个item像是被锁起来了，一直到文件下载完毕
4. 文件被下载完毕了，另一个字段files将会填充结果，这个字段包含一个字典列表（带有下载文件，下载路径，原始url，文件校验和等信息），如果文件下载失败的话，文件不会出现在文件字段中。

### 使用影像管道ImagePipeline
工作流程基本和filepipeline是一致的， 默认字段不是file_urls,files,而是对应image_urls,images.

可以配置缩略图（需要安装pillow包）

### 启用你的Media Pipeline 
启用pipeline是很简单的，在settings文件中使用如下
```json
ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}
ITEM_PIPELINES = {'scrapy.pipelines.files.FilesPipeline': 1}
```
```json
FILES_STORE = '/path/to/valid/dir'
IMAGES_STORE = '/path/to/valid/dir'
```

## 支持的存储

### 文件系统存储
样例url
```
http://www.example.com/image.jpg
```
样例hash1 hash 
```
3afec3b4765f8f0a07b78f98c07b83f013567a0a
```
下载结果会存在哪里
```
<IMAGES_STORE>/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
```
* <IMAGES_STORE> : 这是一个目录，定义在setting里面，指定影像存储位置的。
* full : 这个是个子目录，用于区别缩略图的

### S3存储

设定存储位置
```
IMAGES_STORE = 's3://bucket/images'
```
设置存储策略
```
IMAGES_STORE_S3_ACL = 'public-read'
```
详细ACL可以参考如下： [http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl](http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl)

### 使用样例

1. 启用filepipe或者imagepipeline
2. 爬虫返回的item带有file_urls或者images_url
3. item必须有file_urls,files 或者有image_urls,images

修改默认filepipeline设置
```
FILES_URLS_FIELD = 'field_name_for_your_files_urls'
FILES_RESULT_FIELD = 'field_name_for_your_processed_files'
```
修改默认的imagepipeline设置
```
IMAGES_URLS_FIELD = 'field_name_for_your_images_urls'
IMAGES_RESULT_FIELD = 'field_name_for_your_processed_images'
```

### 额外功能

imagepipe避免下载最近下载过的文件的。 默认是90天的，你可以设置过期日期设置

```
# 120 days of delay for files expiration
FILES_EXPIRES = 120

# 30 days of delay for images expiration
IMAGES_EXPIRES = 30
```

### 缩略图生成
定义如下设置
```json
IMAGES_THUMBS = {
    'small': (50, 50),
    'big': (270, 270),
}
```
最终下载的文件会是这个样子
```
<IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg
```
* size_name : small,big,这个名字是你在IMAGES_THUMBS设置的名字
* image_id  :这个是image_url 经过hash的值


### 过滤小影像
```
IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110
```
这个过滤是过滤那些原始的图片大小， 不会过滤生成缩略图大小变小的文件。如果图片的宽度和高度，任何一个值低于设置的话，就会过滤，1000*45的影像，45<110，这个影像就会被过滤。

### 允许图片的重定向
默认图片等媒体是忽略重定向的， 如果有需要修改如下
```
MEDIA_ALLOW_REDIRECTS = True
```

## 扩展MediaPipelines
### class scrapy.pipelines.files.FilesPipeline
#### get_media_requests(item, info)
这个方法就是遍历下item的file_urls字段，然后请求完成下载，我们可以重写这个方法的
```python
def get_media_requests(self, item, info):
    for file_url in item['file_urls']:
        yield scrapy.Request(file_url)
```
这些请求会被管道处理，一个item对应的所有文件完成下载后进入item_completed方法，一个包含2个元素的元组列表， 这个元组是包含2个元素的， 一个i额是下载状态，一个是详细信息。
样例结果
```json
[(True,
  {'checksum': '2b00042f7481c7b056c4b410d28f33cf',
   'path': 'full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg',
   'url': 'http://www.example.com/files/product1.pdf'}),
 (False,
  Failure(...))]
```
如果返回None就意味着没有文件被下载成功。
#### item_completed(results, item, info)
这个方法在一个item的file_urls字段的所有文件下载完毕的时候触发。
```python
from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    file_paths = [x['path'] for ok, x in results if ok]
    if not file_paths:
        raise DropItem("Item contains no files")
    item['file_paths'] = file_paths
    return item
```
### class scrapy.pipelines.images.ImagesPipeline
这个是继承与FilePipeline的。
#### get_media_requests(item, info)
这个和filepipeline的方法一样， 只是使用的image_urls而不是file_urls.
#### item_completed(results, item, info)
这个和filepipeline的方法一样，只是使用的image_paths而不是file_paths
## 自定义影像pipeLine
```
import scrapy
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem

class MyImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            raise DropItem("Item contains no images")
        item['image_paths'] = image_paths
        return item
```