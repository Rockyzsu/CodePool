- 我们看着真值

表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课

题就是将这个决定参数值的工作交由计算机自动进行。 学习是确定

合适的参数的过程，而人要做的是思考感知机的构造（模型），并把

训练数据交给计算机



- 偏置和权重w1、 w2的作用是不

一样的。具体地说， w1和w2是控制输入信号的重要性的参数，而偏置是调

整神经元被激活的容易程度（输出信号为1的程度）的参数



- 感知机的局限性就在于它只能表示由一条直线分割的空间。图2-8这样弯

曲的曲线无法用感知机表示



- 严格地讲，应该是“单层感知机无法

表示异或门”或者“单层感知机无法分离非线性空间”。接下来，我

们将看到通过组合感知机（叠加层）就可以实现异或门



- 像这样，在异或门的感知机中，工人之间不断进行零件的传送。通过这

样的结构（2层结构），感知机得以实现异或门。这可以解释为“单层感知机

无法表示的东西，通过增加一层就可以解决”。也就是说，通过叠加层（加深

层），感知机能进行更加灵活的表示



- 人们一般会认为计算机内部进行的处理非常复杂，而令人惊讶的是，实

际上只需要通过与非门的组合，就能再现计算机进行的处理



- sigmoid函数的平滑性对神经网络的学习具有重要意义。

- 实际上，上一章介绍的感知机和接下来要介绍

的神经网络的主要区别就在于这个激活函数。



- 使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所

带来的优势，激活函数必须使用非线性函数



- ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输

出0



- 在

神经网络发展的历史上， sigmoid函数很早就开始被使用了，而最近则主要

使用ReLU（Rectifed Linear Unit）函数



- 输出层所用的激活函数，要根据求解问题的性质决定。一般地，回

归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数，

多元分类问题可以使用 softmax函数。关于输出层的激活函数，我

们将在下一节详细介绍



- 经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出

层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。



- 分类问题中使用的softmax函数

exp(x)是表示ex的指数函数（e是纳皮尔常数2.7182 . . .）。式（3.10）表示

假设输出层共有n个神经元，计算第k个神经元的输出yk。如式（3.10）所示，

softmax函数的分子是输入信号ak的指数函数，分母是所有输入信号的指数

函数的和。



- 在进行 softmax 的指数函数的运算时,加上(或者减去)

某个常数并不会改变运算的结果。这里的 C  可以使用任何值,但是为了防

止溢出,一般会使用输入信号中的最大值





- softmax 函数的输出是 0.0 到 1.0 之间的实数。并且,softmax

函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质



- 一般而言,神经网络只把输出值最大的神经元所对应的类别作为识别结果。

并且,即便使用 softmax 函数,输出值最大的神经元的位置也不会变。因此,

神经网络在进行分类时,输出层的 softmax 函数可以省略



- 阶跃函数就像“竹筒敲石”一样,只在某个瞬间产生变化。而 sigmoid 函数,

如图 4-4 所示,不仅函数的输出(竖轴的值)是连续变化的,曲线的斜率(导数)

也是连续变化的。也就是说,sigmoid 函数的导数在任何地方都不为 0。这对

神经网络的学习非常重要。得益于这个斜率不会为 0 的性质,神经网络的学

习得以正确进行



- 我们把这里讨论的有多个变量的函数的导数称为偏导数  

- **梯度指示的方向

是各点处的函数值减小最多的方向 A 。这是一个非常重要的性质,请一定

牢记!** 



- 实验结果表明,学习率过大的话,会发散成一个很大的值;反过来,学

习率过小的话,基本上没怎么更新就结束了。也就是说,设定合适的学习率

是一个很重要的问题。



- 像学习率这样的参数称为超参数。这是一种和神经网络的参数(权重

和偏置)性质不同的参数。相对于神经网络的权重参数是通过训练

数据和学习算法自动获得的,学习率这样的超参数则是人工设定的。

一般来说,超参数需要尝试多个值,以便找到一种可以使学习顺利

进行的设定。



- 后面我们会详细讨论权重参数的初始化,这里只需要知道,权重使用符合高斯

分 布 的 随 机 数 进 行 初 始 化,偏 置 使 用 0 进 行 初 始 化



- epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过

一次时的更新次数



- 实线表示训练数据的识别精度,虚线表示测试数据的识别精

度。如图所示,随着 epoch 的前进(学习的进行),我们发现使用训练数据和

测试数据评价的识别精度都提高了,并且,这两个识别精度基本上没有差异(两

条线基本重叠在一起)。因此,可以说这次的学习中没有发生过拟合的现象。



- 数值微分虽然费时间,但是实现起来很简单。下一章中要实现的稍

微复杂一些的误差反向传播法可以高速地计算梯度



- 这里的第 2 歩“从左向右进行计算”是一种正方向上的传播,简称为正

向传播 (forward propagation)。正向传播是从计算图出发点到结束点的传播。

既然有正向传播这个名称,当然也可以考虑反向(从图上看的话,就是从右向左)

的传播。实际上,这种传播称为反向传播 (backward propagation)。反向传

播将在接下来的导数计算中发挥重要作用



- 几何中,仿射变换包括一次线性变换和一次平移,分别对应神经网络的加权和运算与加偏置运算



- 输入数据为张量(四维数据)的情况



- 数值微分的优点是实现简单,因此,一般情况下不太容易出错。而误差

反向传播法的实现很复杂,容易出错。所以,经常会比较数值微分的结果和

误差反向传播法的结果,以确认误差反向传播法的实现是否正确。确认数值

微分求出的梯度结果和误差反向传播法求出的结果是否一致(严格地讲,是

非常相近)的操作称为梯度确认 (gradient check)



- 如果我们把权重初始值全部设为 0 以减小权重的值,会怎么样呢?从结

论来说,将权重初始值设为 0 不是一个好主意。事实上,将权重初始值设为

0 的话,将无法正确进行学习。



- 这里使用的 sigmoid

函数是 S 型函数,随着输出不断地靠近 0 (或者靠近 1),它的导数的值逐渐接

近 0。因此,偏向 0 和 1 的数据分布会造成反向传播中梯度的值不断变小,最

后消失。这个问题称为梯度消失 (gradient vanishing)。层次加深的深度学习

中,梯度消失的问题可能会更加严重



- 各层的激活值的分布都要求有适当的广度。为什么呢?因为通过

在各层间传递多样性的数据,神经网络可以进行高效的学习。反

过来,如果传递的是有所偏向的数据,就会出现梯度消失或者“表

现力受限”的问题,导致学习可能无法顺利进行。



- 机器学习中经常使用集成学习。所谓集成学习,就是让多个模型单

独进行学习,推理时再取多个模型的输出的平均值。

实验告诉我们,通过进行集成学习,神经网络的识别精度可以提高好几个百分点



- 除了权重和偏置等参数,超参数 (hyper-parameter)也经

常出现。这里所说的超参数是指,比如各层的神经元数量、batch 大小、参

数更新时的学习率或权值衰减等。如果这些超参数没有设置合适的值,模型

的性能就会很差。



- 不能使用测试数据评估超参数的性能。

为什么不能用测试数据评估超参数的性能呢?这是因为如果使用测试数

据调整超参数,超参数的值会对测试数据发生过拟合。换句话说,用测试数

据确认超参数的值的“好坏”,就会导致超参数的值被调整为只拟合测试数据。

这样的话,可能就会得到不能拟合其他数据、泛化能力低的模型。



- 调整超参数时,必须使用超参数专用的确认数据。用于调整超参

数的数据,一般称为验证数据 (validation data)。我们使用这个验证数据来

评估超参数的好坏



- 分割训练数据前,先打乱了输入数据和教师标签。这是因为数据

集的数据可能存在偏向(比如,数据从“0”到“10”按顺序排列等)。

np.random.shuffle(x) 这个函数会改变x的值，重新赋值。



-     permutation = np.random.permutation(x.shape[0]) 返回0到x.shape[0] 的排列

- 有报告 [15] 显示,在进行神经网络的超参数的最优化时,与网格搜索

等有规律的搜索相比,随机采样的搜索方式效果更好。这是因为在

多个超参数中,各个超参数对最终的识别精度的影响程度不同。



- 以上就是超参数的最优化的内容,简单归纳一下,如下所示。

* 步骤 0

设定超参数的范围。

* 步骤 1

从设定的超参数范围中随机采样。

* 步骤 2

使用步骤 1 中采样到的超参数的值进行学习,通过验证数据评估识别精

度(但是要将 epoch 设置得很小) 。

* 步骤 3

重复步骤 1 和步骤 2 (100 次等),根据它们的识别精度的结果,缩小超参

数的范围。



反复进行上述操作,不断缩小超参数的范围,在缩小到一定程度时,从

该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。



*  参 数 的 更 新 方 法,除 了 SGD 之 外,还 有 Momentum、AdaGrad、

Adam 等方法。

* 权重初始值的赋值方法对进行正确的学习非常重要。

*  作为权重初始值,Xavier 初始值、He 初始值等比较有效。

*  通过使用 Batch Normalization,可以加速学习,并且对初始值变得

健壮。

* 抑制过拟合的正则化技术有权值衰减、Dropout 等。

* 逐渐缩小“好值”存在的范围是搜索超参数的一个有效方法



- 全连接层存在什么问题呢?那就是数据的形状被“忽视”了。比如,输

入数据是图像时,图像通常是高、长、通道方向上的 3 维形状。但是,向全

连接层输入时,需要将 3 维数据拉平为 1 维数据。



图像是 3 维形状,这个形状中应该含有重要的空间信息。比如,空间上

邻近的像素为相似的值、RBG 的各个通道之间分别有密切的关联性、相距

较远的像素之间没有什么关联等,3 维形状中可能隐藏有值得提取的本质模

式。但是,因为全连接层会忽视形状,将全部的输入数据作为相同的神经元

(同一维度的神经元)处理,所以无法利用与形状相关的信息。



- 通过填充,大小为 (4, 4) 的输入数据变成了 (6, 6) 的形状。

然后,应用大小为 (3, 3) 的滤波器,生成了大小为 (4, 4) 的输出数据。这个例

子中将填充设成了 1,不过填充的值也可以设置成 2、 3 等任意的整数。在图 7-5

的例子中,如果将填充设为 2,则输入数据的大小变为 (8, 8);如果将填充设

为 3,则大小变为 (10, 10)。

这个填充是指 填充的个数。 填充的内容为0



- 使用填充主要是为了调整输出的大小。比如,对大小为 (4, 4) 的输入

数据应用 (3, 3) 的滤波器时,输出大小变为 (2, 2),相当于输出大小

比输入大小缩小了 2 个元素。这在反复进行多次卷积运算的深度网

络中会成为问题。为什么呢?因为如果每次进行卷积运算都会缩小

空间,那么在某个时刻输出大小就有可能变为 1,导致无法再应用

卷积运算。为了避免出现这样的情况,就要使用填充。在刚才的例

子中,将填充的幅度设为 1,那么相对于输入大小 (4, 4),输出大小

也保持为原来的 (4, 4)。因此,卷积运算就可以在保持空间大小不变

的情况下将数据传给下一层。





- 在 3 维数据的卷积运算中,输入数据和滤波器的通道数

要设为相同的值。在这个例子中,输入数据和滤波器的通道数一致,均为 3。

滤波器大小可以设定为任意值(不过,每个通道的滤波器大小要全部相同)。

这个例子中滤波器大小为 (3, 3),但也可以设定为 (2, 2)、(1, 1)、(5, 5) 等任

意值。再强调一下,通道数只能设定为和输入数据的通道数相同的值(本例

中为 3)。


