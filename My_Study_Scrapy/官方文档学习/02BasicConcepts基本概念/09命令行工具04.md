### view 
用法： scrapy view <url>
需要工程： 否
描述： 打开浏览器打开指定的url
支持一些选项的
* --spider=SPIDER : 强制使用指定的爬虫
* --no-redirect: 不允许重定向

### view源码分析
在scrapy.commands.view.py文件中我们找到如下代码
```python
    def _print_response(self, response, opts):
        open_in_browser(response)
```
然后再scrapy.utils.response.py里面找到如下代码
```python
def open_in_browser(response, _openfunc=webbrowser.open):
    """Open the given response in a local web browser, populating the <base>
    tag for external links to work
    """
    from scrapy.http import HtmlResponse, TextResponse
    # XXX: this implementation is a bit dirty and could be improved
    body = response.body
    if isinstance(response, HtmlResponse):
        if b'<base' not in body:
            repl = '<head><base href="%s">' % response.url
            body = body.replace(b'<head>', to_bytes(repl))
        ext = '.html'
    elif isinstance(response, TextResponse):
        ext = '.txt'
    else:
        raise TypeError("Unsupported response type: %s" %
                        response.__class__.__name__)
    fd, fname = tempfile.mkstemp(ext)
    os.write(fd, body)
    os.close(fd)
    return _openfunc("file://%s" % fname)
```
从上面的文件可以看出来， 获取到响流，然后创建一个temp文件， 写body内容到文件中， 然后浏览器打开这个文件的。

### shell
用法： scrapy shell [url]
需要工程： 否
描述： 启动scrapy的shell，访问url， 这个url 可以是本地的绝对路径，可以使本地的相对路径，也可以远程的，shell的默认是使用ipython，当然可以设置为bpython 或者其他的shell。
这个命令还是用的比较多的， 我们在写爬虫的时候，重要是写具体的css,或者xpath路径， 我们如果不是太确定写的对不对，可以先使用shell这个命令去测试下的。

### shell源码分析
在scrapy.commands.shell.py文件中，我们找到如下代码：
```python
    def run(self, args, opts):
        url = args[0] if args else None
        if url:
            # first argument may be a local file
            url = guess_scheme(url)

        spider_loader = self.crawler_process.spider_loader

        spidercls = DefaultSpider
        if opts.spider:
            spidercls = spider_loader.load(opts.spider)
        elif url:
            spidercls = spidercls_for_request(spider_loader, Request(url),
                                              spidercls, log_multiple=True)

        # The crawler is created this way since the Shell manually handles the
        # crawling engine, so the set up in the crawl method won't work
        crawler = self.crawler_process._create_crawler(spidercls)
        # The Shell class needs a persistent engine in the crawler
        crawler.engine = crawler._create_engine()
        crawler.engine.start()

        self._start_crawler_thread()

        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)
        shell.start(url=url, redirect=not opts.no_redirect)
```
从上面可以看出来，先判定了输入参数，使用spider加载器，加载爬虫，创建爬取者，创建引擎，启动引擎，线程启动爬去这，启动一个shell。

### parse
用法： scrapy parse <url> [options]
需要工程： 是
描述： 使用指定的条件去提取指定的url信息。
支持的选项

    --spider=SPIDER: 强制使用爬虫
    --a NAME=VALUE: 设置爬虫参数
    --callback or -c: 设置爬虫的回调
    --pipelines:设置处理itempipelines
    --rules or -r:使用rules去解析响应流
    --noitems: 不显示抓取的item
    --nolinks: 不提取链接
    --nocolour:避免输出coloul化
    --depth or -d: 请求追踪的深度默认是1.
    --verbose or -v:显示详细信息
### parse源码分析
在scrapy.commands.parse.py文件中，可以发现如下代码
```python
    def run(self, args, opts):
        # parse arguments
        if not len(args) == 1 or not is_url(args[0]):
            raise UsageError()
        else:
            url = args[0]

        # prepare spidercls
        self.set_spidercls(url, opts)

        if self.spidercls and opts.depth > 0:
            self.start_parsing(url, opts)
            self.print_results(opts)
```
根据opts的选项内容，设置下爬虫类属性，开始解析，并根据opts的选项打印指定内容。

###　settings

* 用法： scrapy settings [options]
* 需要工程：否
* 描述：在工程目录的话，显示这个工程的设置信息，不在工程目录，就显示scrapy的默认值。
* 使用样例
```cmd
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
```
### settings源码分析
在scrapy.commands.settings.py文件中，我们可以找到如下代码
```python
  def add_options(self, parser):
        ScrapyCommand.add_options(self, parser)
        parser.add_option("--get", dest="get", metavar="SETTING",
            help="print raw setting value")
        parser.add_option("--getbool", dest="getbool", metavar="SETTING",
            help="print setting value, interpreted as a boolean")
        parser.add_option("--getint", dest="getint", metavar="SETTING",
            help="print setting value, interpreted as an integer")
        parser.add_option("--getfloat", dest="getfloat", metavar="SETTING",
            help="print setting value, interpreted as a float")
        parser.add_option("--getlist", dest="getlist", metavar="SETTING",
            help="print setting value, interpreted as a list")

    def run(self, args, opts):
        settings = self.crawler_process.settings
        if opts.get:
            s = settings.get(opts.get)
            if isinstance(s, BaseSettings):
                print(json.dumps(s.copy_to_dict()))
            else:
                print(s)
        elif opts.getbool:
            print(settings.getbool(opts.getbool))
        elif opts.getint:
            print(settings.getint(opts.getint))
        elif opts.getfloat:
            print(settings.getfloat(opts.getfloat))
        elif opts.getlist:
            print(settings.getlist(opts.getlist))
```
上面可以看出，我们可以使用的选项包括--get,--getbool,--getint,--getfloat, --getlist,这个获取到crawler_process的设置信息，然后获取指定的选项输出出来。

### runspider
* 用法： scrapy runspider <spider_file.py>
* 需要工程： 否
* 描述： 不需要创建一个工程，直接运行一个爬虫文件

### runspider源码分析
在scrapy.commands.runspider.py文件中，找到如下代码：
```python
    def run(self, args, opts):
        if len(args) != 1:
            raise UsageError()
        filename = args[0]
        if not os.path.exists(filename):
            raise UsageError("File not found: %s\n" % filename)
        try:
            module = _import_file(filename)
        except (ImportError, ValueError) as e:
            raise UsageError("Unable to load %r: %s\n" % (filename, e))
        spclasses = list(iter_spider_classes(module))
        if not spclasses:
            raise UsageError("No spider found in file: %s\n" % filename)
        spidercls = spclasses.pop()

        self.crawler_process.crawl(spidercls, **opts.spargs)
        self.crawler_process.start()
```
基本和crawl差不多。

### version
* 用法： scrapy version [-v]
* 需要工程： 否
* 描述： 打印scrapy版本，如果加-v选项也打印python的版本，twisted和平台信息，这在提交bug的时候是非常有效的。

### version源码分析
在scrapy.commands.version.py文件中， 找到如下代码
```python
    def run(self, args, opts):
        if opts.verbose:
            versions = scrapy_components_versions()
            width = max(len(n) for (n, _) in versions)
            patt = "%-{}s : %s".format(width)
            for name, version in versions:
                print(patt % (name, version))
        else:
            print("Scrapy %s" % scrapy.__version__)
```
从上面的代码可以轻松看出来，如果指定-v选项获取到scrapy组件，打印出来组件名字和版本号，否则就直接打印scrapy的版本。
